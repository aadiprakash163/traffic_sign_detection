{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic sign recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### The steps of this project are the following  \n",
    "1). Load Data from the pickle file  \n",
    "2). Visualization of the data provided  \n",
    "3). Preprocessing of dataset  \n",
    "4). Model architecture  \n",
    "5). Train, Validate and Test the model  \n",
    "6). Find accuracy of trained model on test images  \n",
    "7). Test model on new images  \n",
    "8). Summarize model's performnce on new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset provided consists of three pickle files one each for training, testing and validation purpose. They are loaded by opening them in Readbinary mode. There are 34799 training examples, 4410 validation examples and 12630 testing examples.\n",
    "These examples or images belong to one of the 43 classes present in the dataset. However, number of datasets for each class is not same. Histogram below depicts distribution of training examples among different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram](hist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only preprocessing step that i have performed is normalization. I am not converting images to grayscale because i think having three channels can be useful for traffic sign classifications problems. For instance, if we consider classification of traffic light status, then having the RGB channels is mandatory. Also precautionary and warning signs have different coloring scheme and by using three channels we can also have an idea about how crucial that traffic sign is.\n",
    "\n",
    "* Normalisation is done because of the fact that it scales the data in the range 0 to 1 and in this range, activation functions like logsig and tansig has maximum sensitivity so it leads to better convergence of learning. Though no such function is used in this project, it will still improve the learning curve\n",
    "\n",
    "* The training data is also shuffled because dataset provided is grouped according to their classes and training for one class at one time and other at other time will lead to distortions in learning and will not be a good strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Images before and after normalization](preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model architecture as suggested by course instructors, i used Lenet architecture in the begining. It has two convolutional layers but with one fully connected layer instead of two and an output layer. One layer was used earlier to reduce number of weights to be learned.   \n",
    "\n",
    "\n",
    "At first learning was done using unprocessed images and untuned hyper parameters, still it generated fairly good results. I could get validation accuracy of about 0.85 in under 50 epochs. Though initial learning errors were high, they seemed to decrease with every epochs.  \n",
    "\n",
    "Later on, after using normalised images, convergence got faster and tuning of hyper parameters brought required epochs for an accuracy of 0.93 to around 50 which were earlier around 400. An interesting thing that i noticed was that with proper tuning of hyper parameters we can get a very good leap at the first epoch itself. I am consistently getting around 0.80 accuracy at first epoch itself. But still, there is room for a lot of improvement because the learning stagnates after around 10 epochs and further improvement is slow and oscillatory in nature. In this model one problem was that the fully connected layer was reducing 1024 feature maps directly into 43 outputs. Such large reduction is inefficient and should be done in multiple stages.\n",
    "\n",
    "Though this model was workable but by adding one more fully connected layer, number of epochs required to get required accuracy got reduced to just 25.\n",
    "At last i added one more fully connected layer before output layer. It resulted in reduction of epochs from 56 to 25 for an accuracy of 0.95\n",
    "\n",
    "After every convolutional layer we used maxpooling for sub sampling. Sub samppling is used because that we can get rid of not so useful information from the images and only retain the dominant part.\n",
    "\n",
    "Before maxpooling we use ReLU activation, to impart nonlinear classification capability to our classifier.  \n",
    "\n",
    "\n",
    "Dropout is used in both the fully connected layers to prevent overfitting and improve generalization capability of our classifier.\n",
    "Minimum required accuracy is set to 0.95 and iterations are done untill model is not trained to that level i.e. number of epochs is not a hyperparameter here. Details of final model architecture and model training are given below.\n",
    "\n",
    "* The neural network used is a 4-layer Convolutional Neural Network.\n",
    "* It consists of two convolution layer (feature extraction) followed by two fully connected layers (ReLU activation).\n",
    "* Both the convolutional layers(l1 and l2) use a 3X3 filter as their receptive field. First layer's output have 32 channels whereas second convolutional layer outputs 64 channels.\n",
    "* Both convolutional layers are followed by ReLU and Maxpool with kernel size of 2.\n",
    "* Convolutional layers are followed by 2 fully connected layer(fc) with 1024 and 400 nodes.\n",
    "* Connections between fully connected layers have dropout of 0.75.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing of model\n",
    "\n",
    "Training specifics and hyperparameters:\n",
    "* Learning rate: 0.001\n",
    "* Minimum validation accuracy: 0.95\n",
    "* Optimizer used: Adam optimizer\n",
    "* Batch size : 128\n",
    "* Convolution layer kernel size: 5x5\n",
    "* Padding: VALID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer is used because it controls learning rate by itself and also use momentum while learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of epochs: 25  \n",
    "* Validation accuracy: 0.959\n",
    "* Test accuracy: 0.933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing the model with unseen images, 6 images were downloaded from internet. Following test images were downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Test_images/Keep_right.jpg)\n",
    "![title](Test_images/Priority_road.jpg)\n",
    "![title](Test_images/Speed_limit_30.jpg)\n",
    "![title](Test_images/Speed_limit_50.jpg)\n",
    "![title](Test_images/Yield.jpg)\n",
    "![title](Test_images/ZoomedOut_keep_right.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these images are not 32X32, we have to resize them first, normalize and then apply to our classification model.  \n",
    "\n",
    "Our model is classifying 4 out of 6 images correctly. So the crude accuracy on the unseen images is $0.67$ against the obtained test accuraacy of  $ 0.93 $\n",
    "\n",
    "One thing to note from the output of these images is that first and last images correspond to same traffic sign 'Keep right' but the network is classifying the earlier one correctly with full ceratinity but the later one incorrectly.  \n",
    "This might be because of the fact that the later image contains lot of extra stuff other than the traffic sign itself. This causes the unequal amout of image captured in the receptive field in training and testing. And this change in the capture of receptive field causes changes in perception and hence the different results. So, the images with lot of different stuffs than the traffic sign and images with size of the traffic sign different than that in the training set can be particularly difficult for our model to classify. One possible solution to this problem is to crop out the extra stuff out of the image and then apply the model. \n",
    "\n",
    "\n",
    "##### Accuracy with new images: 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible improvements to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier did not give expected accuraacy with new images that means we could not get good generalized training. One possible reason might be unequal distribution of training samples for different classes. As some of the classes have much more samples, training turn out to biased towards those samples and new images are not properly classified.\n",
    "\n",
    "To improve performance of our neural network, we can augment the given dataset with some additional self generated dataset. This will result in training for each class equally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SDCndP1]",
   "language": "python",
   "name": "conda-env-SDCndP1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
